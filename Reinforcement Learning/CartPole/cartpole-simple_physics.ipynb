{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import collections\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# To get smooth animations\n",
    "import matplotlib.animation as animation\n",
    "mpl.rc('animation', html='jshtml')\n",
    "\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Actions / Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_scenario(env, policy, num_frames=500) -> dict:\n",
    "    frames = []\n",
    "    obs_mat = np.empty((num_frames, 4))\n",
    "    actions = np.empty((num_frames,))\n",
    "    rewards = np.empty((num_frames,))\n",
    "    dones = np.empty((num_frames,), dtype=int)\n",
    "    first_done_info = ''\n",
    "    obs = env.reset()  # initial observation\n",
    "    for i in range(num_frames):\n",
    "        action = policy(obs)\n",
    "        obs_mat[i,:] = obs\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        img = env.render(mode=\"rgb_array\")\n",
    "        frames.append(img)\n",
    "        actions[i] = action\n",
    "        rewards[i] = reward\n",
    "        dones[i] = int(done)\n",
    "        if done and first_done_info == '':\n",
    "            first_done_info = info\n",
    "    record = {'frames': frames, 'obs': obs_mat, 'actions': actions, 'rewards': \n",
    "              rewards, 'dones': dones, 'first_done_info':first_done_info}\n",
    "    return record\n",
    "\n",
    "def record_data(env, policy, num_frames=500):\n",
    "    obs_mat = np.empty((num_frames, 4))\n",
    "    actions = np.empty((num_frames,))\n",
    "    rewards = np.empty((num_frames,))\n",
    "    dones = np.empty((num_frames,), dtype=int)\n",
    "    first_done_info = ''\n",
    "    obs = env.reset()  # initial observation\n",
    "    for i in range(num_frames):\n",
    "        action = policy(obs)\n",
    "        obs_mat[i,:] = obs\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        actions[i] = action\n",
    "        rewards[i] = reward\n",
    "        dones[i] = int(done)\n",
    "        if done and first_done_info == '':\n",
    "            first_done_info = info\n",
    "    record = {'obs': obs_mat, 'actions': actions, 'rewards': \n",
    "              rewards, 'dones': dones, 'first_done_info':first_done_info}\n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_scene(num, frames, patch, time_text, obs_mat, actions, cum_rewards, dones):\n",
    "    patch.set_data(frames[num])\n",
    "    text = f\"frame: {num}\"\n",
    "    text += \", Obs: ({:.3f}, {:.3f}, {:.3f}, {:.3f})\\n\".format(*obs_mat[num,:])\n",
    "    text += f\"Action: {actions[num]}\"\n",
    "    text += f\", Cumulative Reward: {cum_rewards[num]}\"\n",
    "    text += f\", Done: {dones[num]}\"\n",
    "    time_text.set_text(text)\n",
    "    return patch, time_text\n",
    "\n",
    "def plot_animation(record, repeat=False, interval=40):\n",
    "    '''record should contain\n",
    "    frames: list of N frames\n",
    "    obs: (N, 4) array of observations\n",
    "    actions: (N, ) array of actions {0, 1}\n",
    "    rewards: (N, ) array of rewards at each step {0, 1}\n",
    "    dones: (N, 1) array of dones {0, 1}\n",
    "    '''\n",
    "    cum_rewards = np.cumsum(record['rewards'])\n",
    "    frames = record['frames']\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(record['frames'][0])\n",
    "    ax = plt.gca()\n",
    "    time_text = ax.text(0., 0.95,'',horizontalalignment='left',verticalalignment='top', transform=ax.transAxes)\n",
    "    plt.axis('off')\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, update_scene, fargs=(frames, patch, time_text, record['obs'], record['actions'], cum_rewards, record['dones']),\n",
    "        frames=len(frames), repeat=repeat, interval=interval)\n",
    "    plt.close()\n",
    "    return anim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_scenario = 1000\n",
    "MAX_ACTIONS = 500\n",
    "\n",
    "def test_policy(policy_func, n_scenario = N_scenario, max_actions = MAX_ACTIONS, verbose=False):\n",
    "    final_rewards = []\n",
    "    for episode in range(n_scenario):\n",
    "        if verbose and episode % 50 == 0:\n",
    "            print(episode)\n",
    "        episode_rewards = 0\n",
    "        obs = env.reset()  # reset to a random position\n",
    "        for step in range(max_actions):\n",
    "            action = policy_func(obs)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_rewards += reward\n",
    "            if done:\n",
    "                break\n",
    "        final_rewards.append(episode_rewards)\n",
    "    return final_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_policy(final_rewards, policy_name:str=''):\n",
    "    fig = plt.plot(range(len(final_rewards)), final_rewards)\n",
    "    plt.grid()\n",
    "    plt.title(policy_name + \" Mean Reward {:.2f}, Std Reward {:.2f}\".format(np.mean(final_rewards), np.min(final_rewards)))\n",
    "    plt.ylabel('Cum Reward')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylim(0, max(final_rewards)*1.1)\n",
    "    return fig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combines policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def theta_omega_policy(obs):\n",
    "    theta, w = obs[2:4]\n",
    "    if abs(theta) < 0.03:\n",
    "        return 0 if w < 0 else 1\n",
    "    else:\n",
    "        return 0 if theta < 0 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.seed(42)\n",
    "random.seed(0)\n",
    "\n",
    "# the cart-pole experiment will end if it lasts more than 500 steps, with info=\"'TimeLimit.truncated': True\"\n",
    "theta_omega_rewards = test_policy(theta_omega_policy, max_actions=510)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_policy(theta_omega_rewards, \"Theta-Omega Policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_omega_record = record_scenario(env, theta_omega_policy, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_animation(theta_omega_record)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a decision tree to learn the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "obs = theta_omega_record['obs']\n",
    "actions = theta_omega_record['actions']\n",
    "rewards = theta_omega_record['rewards']\n",
    "\n",
    "print(len(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(max_depth=3,class_weight='balanced')\n",
    "mlp = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(15,9), random_state=10, max_iter=100000, tol=1e-6)\n",
    "clf.fit(obs, actions)\n",
    "mlp.fit(obs, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(actions, clf.predict(obs)), accuracy_score(actions, clf.predict(obs)))\n",
    "print(f1_score(actions, mlp.predict(obs)), accuracy_score(actions, mlp.predict(obs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=300)\n",
    "# plot_tree(clf, feature_names=['x', 'v', r'$\\theta$', r'$\\omega$'], class_names=[\"left\",\"right\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[actions == 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = mlp.predict(obs)\n",
    "speed = obs[:, 1]\n",
    "theta = obs[:, 2]\n",
    "plt.figure(figsize=(12, 8), dpi=300)\n",
    "plt.scatter(theta, speed, c=color, s=2), print(obs[:, 2].shape)\n",
    "plt.vlines([0.03, -0.03], ymin=min(speed), ymax=max(speed))\n",
    "plt.hlines([-0.032], xmin=min(theta), xmax=max(theta))\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('speed')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f523f7c76dd18e7ed336217f32f6f704c23c323644912475b9d3570cf04b060"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
