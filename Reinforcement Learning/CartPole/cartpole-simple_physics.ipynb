{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import collections\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# To get smooth animations\n",
    "import matplotlib.animation as animation\n",
    "mpl.rc('animation', html='jshtml')\n",
    "\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Actions / Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_scenario(env, policy, num_frames=500) -> dict:\n",
    "    frames = []\n",
    "    obs_mat = np.empty((num_frames, 4))\n",
    "    actions = np.empty((num_frames,))\n",
    "    rewards = np.empty((num_frames,))\n",
    "    dones = np.empty((num_frames,), dtype=int)\n",
    "    first_done_info = ''\n",
    "    obs = env.reset()  # initial observation\n",
    "    for i in range(num_frames):\n",
    "        action = policy(obs)\n",
    "        obs_mat[i,:] = obs\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        img = env.render(mode=\"rgb_array\")\n",
    "        frames.append(img)\n",
    "        actions[i] = action\n",
    "        rewards[i] = reward\n",
    "        dones[i] = int(done)\n",
    "        if done and first_done_info == '':\n",
    "            first_done_info = info\n",
    "    record = {'frames': frames, 'obs': obs_mat, 'actions': actions, 'rewards': \n",
    "              rewards, 'dones': dones, 'first_done_info':first_done_info}\n",
    "    return record\n",
    "\n",
    "def record_data(env, policy, num_frames=500):\n",
    "    obs_mat = np.empty((num_frames, 4))\n",
    "    actions = np.empty((num_frames,))\n",
    "    rewards = np.empty((num_frames,))\n",
    "    dones = np.empty((num_frames,), dtype=int)\n",
    "    first_done_info = ''\n",
    "    obs = env.reset()  # initial observation\n",
    "    for i in range(num_frames):\n",
    "        action = policy(obs)\n",
    "        obs_mat[i,:] = obs\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        actions[i] = action\n",
    "        rewards[i] = reward\n",
    "        dones[i] = int(done)\n",
    "        if done and first_done_info == '':\n",
    "            first_done_info = info\n",
    "    record = {'obs': obs_mat, 'actions': actions, 'rewards': \n",
    "              rewards, 'dones': dones, 'first_done_info':first_done_info}\n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_scene(num, frames, patch, time_text, obs_mat, actions, cum_rewards, dones):\n",
    "    patch.set_data(frames[num])\n",
    "    text = f\"frame: {num}\"\n",
    "    text += \", Obs: ({:.3f}, {:.3f}, {:.3f}, {:.3f})\\n\".format(*obs_mat[num,:])\n",
    "    text += f\"Action: {actions[num]}\"\n",
    "    text += f\", Cumulative Reward: {cum_rewards[num]}\"\n",
    "    text += f\", Done: {dones[num]}\"\n",
    "    time_text.set_text(text)\n",
    "    return patch, time_text\n",
    "\n",
    "def plot_animation(record, repeat=False, interval=40):\n",
    "    '''record should contain\n",
    "    frames: list of N frames\n",
    "    obs: (N, 4) array of observations\n",
    "    actions: (N, ) array of actions {0, 1}\n",
    "    rewards: (N, ) array of rewards at each step {0, 1}\n",
    "    dones: (N, 1) array of dones {0, 1}\n",
    "    '''\n",
    "    cum_rewards = np.cumsum(record['rewards'])\n",
    "    frames = record['frames']\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(record['frames'][0])\n",
    "    ax = plt.gca()\n",
    "    time_text = ax.text(0., 0.95,'',horizontalalignment='left',verticalalignment='top', transform=ax.transAxes)\n",
    "    plt.axis('off')\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, update_scene, fargs=(frames, patch, time_text, record['obs'], record['actions'], cum_rewards, record['dones']),\n",
    "        frames=len(frames), repeat=repeat, interval=interval)\n",
    "    plt.close()\n",
    "    return anim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_scenario = 1000\n",
    "MAX_ACTIONS = 500\n",
    "\n",
    "def test_policy(policy_func, n_scenario = N_scenario, max_actions = MAX_ACTIONS, verbose=False):\n",
    "    final_rewards = []\n",
    "    for episode in range(n_scenario):\n",
    "        if verbose and episode % 50 == 0:\n",
    "            print(episode)\n",
    "        episode_rewards = 0\n",
    "        obs = env.reset()  # reset to a random position\n",
    "        for step in range(max_actions):\n",
    "            action = policy_func(obs)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_rewards += reward\n",
    "            if done:\n",
    "                break\n",
    "        final_rewards.append(episode_rewards)\n",
    "    return final_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_policy(final_rewards, policy_name:str=''):\n",
    "    fig = plt.plot(range(len(final_rewards)), final_rewards)\n",
    "    plt.grid()\n",
    "    plt.title(policy_name + \" Mean Reward {:.2f}, Std Reward {:.2f}\".format(np.mean(final_rewards), np.min(final_rewards)))\n",
    "    plt.ylabel('Cum Reward')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylim(0, max(final_rewards)*1.1)\n",
    "    return fig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combines policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def theta_omega_policy(obs):\n",
    "    theta, w = obs[2:4]\n",
    "    if abs(theta) < 0.032:\n",
    "        return 0 if w < 0 else 1\n",
    "    else:\n",
    "        return 0 if theta < 0 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.seed(42)\n",
    "random.seed(0)\n",
    "\n",
    "# the cart-pole experiment will end if it lasts more than 500 steps, with info=\"'TimeLimit.truncated': True\"\n",
    "theta_omega_rewards = test_policy(theta_omega_policy, max_actions=510)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_policy(theta_omega_rewards, \"Theta-Omega Policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_omega_record = record_data(env, theta_omega_policy, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_animation(theta_omega_record)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a decision tree to learn the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, DecisionTreeRegressor\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "obs, actions, rewards = [], [], []\n",
    "\n",
    "n_records = 100\n",
    "for i in range(n_records):\n",
    "    theta_omega_record = record_data(env, theta_omega_policy, 500)\n",
    "    obs.append(theta_omega_record['obs'])\n",
    "    actions.append(theta_omega_record['actions'])\n",
    "    rewards.append(theta_omega_record['rewards'])\n",
    "\n",
    "rewards = np.array(rewards).ravel()\n",
    "obs = np.array(obs).reshape(n_records*500, 4)\n",
    "actions = np.array(actions).ravel()\n",
    "\n",
    "\n",
    "print(obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(max_depth=4,class_weight='balanced')\n",
    "dtr = DecisionTreeRegressor(max_depth=4)\n",
    "lbm = LGBMRegressor()\n",
    "lbc = LGBMClassifier(n_estimators=200)\n",
    "\n",
    "mlp = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(15,9), random_state=10, max_iter=100000, tol=1e-6)\n",
    "clf.fit(obs, actions)\n",
    "# mlp.fit(obs, actions)\n",
    "dtr.fit(obs, actions)\n",
    "lbm.fit(obs, actions)\n",
    "lbc.fit(obs, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(actions, clf.predict(obs)), accuracy_score(actions, clf.predict(obs)))\n",
    "print(f1_score(actions, lbc.predict(obs)), accuracy_score(actions, lbc.predict(obs)))\n",
    "#dtr.predict(obs)\n",
    "lbc.predict(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=300, figsize=(4,3));\n",
    "# plot_tree(clf, feature_names=['x', 'v', r'$\\theta$', r'$\\omega$'], class_names=[\"left\",\"right\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[actions == 0].shape\n",
    "env.seed(0)\n",
    "env.reset(), env.seed(0), env.reset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Montecarlo-like\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obs = []\n",
    "actions = []\n",
    "seed=37\n",
    "\n",
    "env.seed(seed)\n",
    "env.reset()\n",
    "\n",
    "step=0\n",
    "while True:\n",
    "    step+=1\n",
    "    action = np.random.choice([0,1])\n",
    "    ob, rewards, terminated, info = env.step(action)\n",
    "    actions.append(action)\n",
    "    if terminated: break\n",
    "\n",
    "print(len(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_actions(env, change_node, actions):\n",
    "    env.seed(seed)\n",
    "    env.reset()\n",
    "    obs = []\n",
    "    new_actions = []\n",
    "    step=0\n",
    "    for action in actions[:-change_node]:\n",
    "        step+=1\n",
    "        # action = np.random.choice([0,1])\n",
    "        ob, rewards, terminated, info = env.step(action)\n",
    "        obs.append(ob), new_actions.append(action)\n",
    "        if terminated: break\n",
    "\n",
    "    if not terminated:\n",
    "        # print(\"continue to explore\", len(new_actions))\n",
    "        while True:\n",
    "            step+=1\n",
    "            action = np.random.choice([0,1])\n",
    "            ob, rewards, terminated, info = env.step(action)\n",
    "            obs.append(ob), new_actions.append(action)\n",
    "            if terminated: break\n",
    "\n",
    "    return new_actions, obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_actions=[]\n",
    "best_obs = []\n",
    "for ep in range(30):\n",
    "    print(f\" {ep} \".center(80, '*'))\n",
    "    change_node=1\n",
    "    if len(actions) == 500:\n",
    "        break\n",
    "    if len(best_actions)>0:\n",
    "        actions = best_actions\n",
    "    best_actions = []\n",
    "    while change_node<len(actions):\n",
    "        new_actions, _ = explore_actions(env, change_node, actions)\n",
    "        if len(new_actions)>len(actions):\n",
    "            #print(len(new_actions), len(actions), change_node)\n",
    "            if len(new_actions)>len(best_actions):\n",
    "                best_actions=new_actions\n",
    "                # best_obs = obs\n",
    "        change_node+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.seed(seed)\n",
    "env.reset()\n",
    "obs = []\n",
    "step=0\n",
    "for action in actions[:]:\n",
    "    step+=1\n",
    "    # action = np.random.choice([0,1])\n",
    "    ob, rewards, terminated, info = env.step(action)\n",
    "    obs.append(ob)\n",
    "    if terminated: break\n",
    "\n",
    "obs = np.array(obs)\n",
    "#obs.shape, new_obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_obs = np.concatenate([obs, new_obs])\n",
    "x_obs.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on random montecarlo search\n",
    "Inefficient random search -> give a weight to each node to increase the efficiency of the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(max_depth=10,class_weight='balanced')\n",
    "clf.fit(obs, actions)\n",
    "accuracy_score(actions, clf.predict(obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color = clf.predict(obs)\n",
    "color = actions\n",
    "speed = obs[:, 3]\n",
    "theta = obs[:, 2]\n",
    "plt.figure(figsize=(6, 4), dpi=200)\n",
    "plt.scatter(theta, speed, c=color, s=1), print(obs[:, 2].shape)\n",
    "plt.vlines([0.032, -0.032], ymin=min(speed), ymax=max(speed))\n",
    "plt.hlines([0], xmin=min(theta), xmax=max(theta))\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('omega')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f523f7c76dd18e7ed336217f32f6f704c23c323644912475b9d3570cf04b060"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
