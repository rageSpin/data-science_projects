{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import collections\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# To get smooth animations\n",
    "import matplotlib.animation as animation\n",
    "mpl.rc('animation', html='jshtml')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Actions / Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_scenario(env, policy, num_frames=100) -> dict:\n",
    "    frames = []\n",
    "    obs_mat = np.empty((num_frames, 4))\n",
    "    actions = np.empty((num_frames,))\n",
    "    rewards = np.empty((num_frames,))\n",
    "    dones = np.empty((num_frames,), dtype=int)\n",
    "    first_done_info = ''\n",
    "    obs = env.reset()  # initial observation\n",
    "    for i in range(num_frames):\n",
    "        action = policy(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        img = env.render(mode=\"rgb_array\")\n",
    "        frames.append(img)\n",
    "        obs_mat[i,:] = obs\n",
    "        actions[i] = action\n",
    "        rewards[i] = reward\n",
    "        dones[i] = int(done)\n",
    "        if done and first_done_info == '':\n",
    "            first_done_info = info\n",
    "    record = {'frames': frames, 'obs': obs_mat, 'actions': actions, 'rewards': \n",
    "              rewards, 'dones': dones, 'first_done_info':first_done_info}\n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_scene(num, frames, patch, time_text, obs_mat, actions, cum_rewards, dones):\n",
    "    patch.set_data(frames[num])\n",
    "    text = f\"frame: {num}\"\n",
    "    text += \", Obs: ({:.3f}, {:.3f}, {:.3f}, {:.3f})\\n\".format(*obs_mat[num,:])\n",
    "    text += f\"Action: {actions[num]}\"\n",
    "    text += f\", Cumulative Reward: {cum_rewards[num]}\"\n",
    "    text += f\", Done: {dones[num]}\"\n",
    "    time_text.set_text(text)\n",
    "    return patch, time_text\n",
    "\n",
    "def plot_animation(record, repeat=False, interval=40):\n",
    "    '''record should contain\n",
    "    frames: list of N frames\n",
    "    obs: (N, 4) array of observations\n",
    "    actions: (N, ) array of actions {0, 1}\n",
    "    rewards: (N, ) array of rewards at each step {0, 1}\n",
    "    dones: (N, 1) array of dones {0, 1}\n",
    "    '''\n",
    "    cum_rewards = np.cumsum(record['rewards'])\n",
    "    frames = record['frames']\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(record['frames'][0])\n",
    "    ax = plt.gca()\n",
    "    time_text = ax.text(0., 0.95,'',horizontalalignment='left',verticalalignment='top', transform=ax.transAxes)\n",
    "    plt.axis('off')\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, update_scene, fargs=(frames, patch, time_text, record['obs'], record['actions'], cum_rewards, record['dones']),\n",
    "        frames=len(frames), repeat=repeat, interval=interval)\n",
    "    plt.close()\n",
    "    return anim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy(policy_func, n_scenario = N_scenario, max_actions = MAX_ACTIONS, verbose=False):\n",
    "    final_rewards = []\n",
    "    for episode in range(n_scenario):\n",
    "        if verbose and episode % 50 == 0:\n",
    "            print(episode)\n",
    "        episode_rewards = 0\n",
    "        obs = env.reset()  # reset to a random position\n",
    "        for step in range(max_actions):\n",
    "            action = policy_func(obs)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_rewards += reward\n",
    "            if done:\n",
    "                break\n",
    "        final_rewards.append(episode_rewards)\n",
    "    return final_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_policy(final_rewards, policy_name:str=''):\n",
    "    fig = plt.plot(range(len(final_rewards)), final_rewards)\n",
    "    plt.grid()\n",
    "    plt.title(policy_name + \" Mean Reward {:.2f}, Std Reward {:.2f}\".format(np.mean(final_rewards), np.min(final_rewards)))\n",
    "    plt.ylabel('Cum Reward')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylim(0, max(final_rewards)*1.1)\n",
    "    return fig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combines policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "929ea3a73902e04b651b50d7e8bff86e69e28c3f97df2b308a538a481df81bfd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
