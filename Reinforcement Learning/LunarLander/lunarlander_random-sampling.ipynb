{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import collections\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best random sampling\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obs = []\n",
    "def get_random_actions(env, seed):\n",
    "    actions = []\n",
    "\n",
    "    env.seed(seed)\n",
    "    env.reset()\n",
    "\n",
    "    step=0\n",
    "    while True:\n",
    "        step+=1\n",
    "        action = env.action_space.sample()\n",
    "        ob, rewards, terminated, info = env.step(action)\n",
    "        actions.append(action)\n",
    "        if terminated: break\n",
    "    \n",
    "    return actions\n",
    "\n",
    "def get_random_rewards(env, seed):\n",
    "    actions = []\n",
    "    rewards = []\n",
    "\n",
    "    env.seed(seed)\n",
    "    env.reset()\n",
    "\n",
    "    step=0\n",
    "    while True:\n",
    "        step+=1\n",
    "        action = env.action_space.sample()\n",
    "        ob, reward, terminated, info = env.step(action)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        if terminated: \n",
    "            break\n",
    "    \n",
    "    return rewards, actions\n",
    "\n",
    "\n",
    "def explore_rewards(env, change_node, actions, old_rewards):\n",
    "    env.seed(seed)\n",
    "    env.reset()\n",
    "\n",
    "    new_actions = []\n",
    "    rewards = old_rewards[:-change_node]\n",
    "    \n",
    "    step=0\n",
    "    for action in actions[:-change_node]:\n",
    "        step+=1\n",
    "        ob, reward, terminated, info = env.step(action)\n",
    "        new_actions.append(action)\n",
    "        if terminated: break\n",
    "\n",
    "    if not terminated:\n",
    "        # print(\"continue to explore\", len(new_actions))\n",
    "        while True:\n",
    "            step+=1\n",
    "            action = env.action_space.sample()\n",
    "            ob, reward, terminated, info = env.step(action)\n",
    "            new_actions.append(action), rewards.append(reward)\n",
    "            if terminated: break\n",
    "\n",
    "    return rewards, new_actions\n",
    "    \n",
    "def explore_actions(env, change_node, actions):\n",
    "    env.seed(seed)\n",
    "    env.reset()\n",
    "\n",
    "    new_actions = []\n",
    "\n",
    "    step=0\n",
    "    for action in actions[:-change_node]:\n",
    "        step+=1\n",
    "        ob, rewards, terminated, info = env.step(action)\n",
    "        new_actions.append(action)\n",
    "        if terminated: break\n",
    "\n",
    "    if not terminated:\n",
    "        # print(\"continue to explore\", len(new_actions))\n",
    "        while True:\n",
    "            step+=1\n",
    "            action = env.action_space.sample()\n",
    "            ob, rewards, terminated, info = env.step(action)\n",
    "            new_actions.append(action)\n",
    "            if terminated: break\n",
    "\n",
    "    return new_actions\n",
    "\n",
    "\n",
    "def find_best_rewards(n, env, actions, rewards):\n",
    "    best_actions=[]\n",
    "    best_rewards = []\n",
    "    for ep in range(n):\n",
    "        print(f\" {ep} \".center(80, '*'))\n",
    "        change_node=1\n",
    "        # if len(actions) == 500:\n",
    "        #     return actions\n",
    "        if len(best_actions)>0:\n",
    "            actions = best_actions\n",
    "        if len(best_rewards)>1:\n",
    "            rewards = best_rewards\n",
    "        best_actions = []\n",
    "        best_rewards = [-1e6] \n",
    "        while change_node<len(actions) and change_node < 100:\n",
    "            new_rewards, new_actions = explore_rewards(env, change_node, actions, rewards)\n",
    "            if sum(new_rewards)>sum(best_rewards):\n",
    "                # print(len(new_actions), len(actions), change_node)\n",
    "                #if len(new_actions)>len(best_actions):\n",
    "                print(len(new_actions), len(actions), change_node, sum(new_rewards))\n",
    "                best_actions=new_actions\n",
    "                best_rewards=new_rewards\n",
    "            change_node+=1\n",
    "        \n",
    "    return best_actions, best_rewards if len(best_actions)>len(actions) else actions\n",
    "\n",
    "\n",
    "def get_obs(env, actions_result):\n",
    "    obs_res = []\n",
    "    for key in actions_result:\n",
    "        env.seed(key)\n",
    "        ob = env.reset()\n",
    "        actions = actions_result[key]\n",
    "        obs = []\n",
    "        for action in actions:\n",
    "            obs.append(ob)\n",
    "            ob, rewards, terminated, info = env.step(action)\n",
    "            \n",
    "            if terminated: break\n",
    "        \n",
    "        obs_res.append(np.array(obs))\n",
    "    \n",
    "    return obs_res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************** 0 ***************************************\n",
      "102 102 1 -149.00837859088466\n",
      "102 102 5 -141.56807267363666\n",
      "102 102 16 -128.0574026152969\n",
      "102 102 24 -125.55624193964569\n",
      "104 102 28 -123.26043672630061\n",
      "103 102 30 -115.39515325051048\n",
      "124 102 32 -13.980617247773807\n",
      "120 102 34 10.231381970022568\n",
      "************************************** 1 ***************************************\n",
      "120 120 1 10.231381970022568\n",
      "120 120 4 11.686774316464266\n",
      "1000 120 34 73.23397092383432\n",
      "************************************** 2 ***************************************\n",
      "1000 1000 1 73.60809107572727\n",
      "1000 1000 3 75.35113808488286\n",
      "1000 1000 9 77.36513234831155\n",
      "1000 1000 13 77.63069625639572\n",
      "1000 1000 16 78.33312204631495\n",
      "1000 1000 59 78.48170719275846\n"
     ]
    }
   ],
   "source": [
    "seed=0\n",
    "rewards, actions = get_random_rewards(env, seed)\n",
    "\n",
    "new_r, new_act = explore_rewards(env, 50, actions, rewards)\n",
    "# print(sum(new_r), sum(rewards))\n",
    "best_actions, best_rewards = find_best_rewards(3, env, actions, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\stefano.giannini_ama\\Documents\\Python\\Learn\\data-science_projects\\Reinforcement Learning\\LunarLander\\lunarlander_random-sampling.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/stefano.giannini_ama/Documents/Python/Learn/data-science_projects/Reinforcement%20Learning/LunarLander/lunarlander_random-sampling.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     step\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/stefano.giannini_ama/Documents/Python/Learn/data-science_projects/Reinforcement%20Learning/LunarLander/lunarlander_random-sampling.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     env\u001b[39m.\u001b[39mrender()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/stefano.giannini_ama/Documents/Python/Learn/data-science_projects/Reinforcement%20Learning/LunarLander/lunarlander_random-sampling.ipynb#W4sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.2\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/stefano.giannini_ama/Documents/Python/Learn/data-science_projects/Reinforcement%20Learning/LunarLander/lunarlander_random-sampling.ipynb#W4sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mif\u001b[39;00m terminated: \u001b[39mbreak\u001b[39;00m\u001b[39m#steps.append(step);break\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/stefano.giannini_ama/Documents/Python/Learn/data-science_projects/Reinforcement%20Learning/LunarLander/lunarlander_random-sampling.ipynb#W4sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m env\u001b[39m.\u001b[39mclose()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "step=0\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env.seed(seed)\n",
    "env.reset()\n",
    "\n",
    "while True:\n",
    "    # print(ob.reshape(1, -1).shape)\n",
    "    action = best_actions[step]\n",
    "    # action = res[seeds[4]][step]\n",
    "    ob, reward, terminated, info = env.step(action)\n",
    "    step+=1\n",
    "    env.render()\n",
    "    time.sleep(0.05)\n",
    "    print(step)\n",
    "    if terminated: break#steps.append(step);break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'find_best_actions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\stefano.giannini_ama\\Documents\\Python\\Learn\\data-science_projects\\Reinforcement Learning\\LunarLander\\lunarlander_random-sampling.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/stefano.giannini_ama/Documents/Python/Learn/data-science_projects/Reinforcement%20Learning/LunarLander/lunarlander_random-sampling.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m seed \u001b[39min\u001b[39;00m seeds[:\u001b[39m2\u001b[39m]:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/stefano.giannini_ama/Documents/Python/Learn/data-science_projects/Reinforcement%20Learning/LunarLander/lunarlander_random-sampling.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     actions \u001b[39m=\u001b[39m get_random_actions(env, seed)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/stefano.giannini_ama/Documents/Python/Learn/data-science_projects/Reinforcement%20Learning/LunarLander/lunarlander_random-sampling.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     best_actions \u001b[39m=\u001b[39m find_best_actions(env, actions)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/stefano.giannini_ama/Documents/Python/Learn/data-science_projects/Reinforcement%20Learning/LunarLander/lunarlander_random-sampling.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[\u001b[39m\u001b[39m{\u001b[39;00mseed\u001b[39m}\u001b[39;00m\u001b[39m] Actions length improvement:\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39mlen\u001b[39m(actions), \u001b[39m\"\u001b[39m\u001b[39m->\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(best_actions))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/stefano.giannini_ama/Documents/Python/Learn/data-science_projects/Reinforcement%20Learning/LunarLander/lunarlander_random-sampling.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     res[seed] \u001b[39m=\u001b[39m best_actions\n",
      "\u001b[1;31mNameError\u001b[0m: name 'find_best_actions' is not defined"
     ]
    }
   ],
   "source": [
    "res = {}\n",
    "seeds = [0, 1, 5, 10, 21, 42, 47, 63, 84, 100, 121, 144]\n",
    "for seed in seeds[:2]:\n",
    "    actions = get_random_actions(env, seed)\n",
    "    # best_actions = find_best_actions(env, actions)\n",
    "    # print(f\"[{seed}] Actions length improvement:\",len(actions), \"->\", len(best_actions))\n",
    "    # res[seed] = best_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = get_obs(env, res)\n",
    "array_obs = np.concatenate(obs[:])\n",
    "array_obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_actions = np.concatenate([np.array(act[1]) for act in res.items()][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "color = array_actions\n",
    "speed = array_obs[:, 3]\n",
    "theta = array_obs[:, 2]\n",
    "plt.figure(figsize=(7, 3), dpi=200)\n",
    "plt.scatter(theta, speed, c=color, s=1), #print(obs[:, 2].shape)\n",
    "plt.vlines([0.032, -0.032], ymin=min(speed), ymax=max(speed))\n",
    "plt.hlines([0], xmin=min(theta), xmax=max(theta))\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('omega')\n",
    "plt.colorbar();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lbc = LGBMClassifier(n_estimators=100)\n",
    "\n",
    "lbc.fit(array_obs, array_actions)\n",
    "accuracy_score(array_actions, lbc.predict(array_obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbc.feature_importances_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model on environment\n",
    "It suuucks, I saved *next_obs* and not the actual obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = []\n",
    "for seed in range(1, 200, 5):\n",
    "    env.seed(seed)\n",
    "    ob = env.reset()\n",
    "\n",
    "    step = 0\n",
    "    while True:\n",
    "        # print(ob.reshape(1, -1).shape)\n",
    "        action = lbc.predict(ob.reshape(1, -1))[0]\n",
    "        # action = res[seeds[4]][step]\n",
    "        ob, reward, terminated, info = env.step(action)\n",
    "        step+=1\n",
    "        if terminated: steps.append(step);break\n",
    "        \n",
    "    print(seed, ':',step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(steps), np.mean(steps), np.min(steps), np.max(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "929ea3a73902e04b651b50d7e8bff86e69e28c3f97df2b308a538a481df81bfd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
