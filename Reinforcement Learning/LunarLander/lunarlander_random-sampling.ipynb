{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import collections\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best random sampling\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obs = []\n",
    "def get_random_actions(env, seed):\n",
    "    actions = []\n",
    "\n",
    "    env.seed(seed)\n",
    "    env.reset()\n",
    "\n",
    "    step=0\n",
    "    while True:\n",
    "        step+=1\n",
    "        action = env.action_space.sample()\n",
    "        ob, rewards, terminated, info = env.step(action)\n",
    "        actions.append(action)\n",
    "        if terminated: break\n",
    "    \n",
    "    return actions\n",
    "\n",
    "\n",
    "def explore_actions(env, change_node, actions):\n",
    "    env.seed(seed)\n",
    "    env.reset()\n",
    "\n",
    "    new_actions = []\n",
    "    step=0\n",
    "    for action in actions[:-change_node]:\n",
    "        step+=1\n",
    "        ob, rewards, terminated, info = env.step(action)\n",
    "        new_actions.append(action)\n",
    "        if terminated: break\n",
    "\n",
    "    if not terminated:\n",
    "        # print(\"continue to explore\", len(new_actions))\n",
    "        while True:\n",
    "            step+=1\n",
    "            action = env.action_space.sample()\n",
    "            ob, rewards, terminated, info = env.step(action)\n",
    "            new_actions.append(action)\n",
    "            if terminated: break\n",
    "\n",
    "    return new_actions\n",
    "\n",
    "\n",
    "def find_best_actions(env, actions):\n",
    "    best_actions=[]\n",
    "    best_obs = []\n",
    "    for ep in range(30):\n",
    "        # print(f\" {ep} \".center(80, '*'))\n",
    "        change_node=1\n",
    "        if len(actions) == 500:\n",
    "            return actions\n",
    "        if len(best_actions)>0:\n",
    "            actions = best_actions\n",
    "        best_actions = []\n",
    "        while change_node<len(actions):\n",
    "            new_actions = explore_actions(env, change_node, actions)\n",
    "            if len(new_actions)>len(actions):\n",
    "                # print(len(new_actions), len(actions), change_node)\n",
    "                if len(new_actions)>len(best_actions):\n",
    "                    # print(len(new_actions), len(actions), change_node)\n",
    "                    best_actions=new_actions\n",
    "            change_node+=1\n",
    "        \n",
    "    return best_actions if len(best_actions)>len(actions) else actions\n",
    "\n",
    "\n",
    "def get_obs(env, actions_result):\n",
    "    obs_res = []\n",
    "    for key in actions_result:\n",
    "        env.seed(key)\n",
    "        ob = env.reset()\n",
    "        actions = actions_result[key]\n",
    "        obs = []\n",
    "        for action in actions:\n",
    "            obs.append(ob)\n",
    "            ob, rewards, terminated, info = env.step(action)\n",
    "            \n",
    "            if terminated: break\n",
    "        \n",
    "        obs_res.append(np.array(obs))\n",
    "    \n",
    "    return obs_res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = 10\n",
    "# actions = get_random_actions(env, seed)\n",
    "# best_actions = find_best_actions(env, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = get_random_actions(env, seed)\n",
    "len(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\stefano.giannini_ama\\Documents\\Python\\Learn\\data-science_projects\\Reinforcement Learning\\LunarLander\\lunarlander_random-sampling.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/stefano.giannini_ama/Documents/Python/Learn/data-science_projects/Reinforcement%20Learning/LunarLander/lunarlander_random-sampling.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m seed \u001b[39min\u001b[39;00m seeds[:\u001b[39m2\u001b[39m]:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/stefano.giannini_ama/Documents/Python/Learn/data-science_projects/Reinforcement%20Learning/LunarLander/lunarlander_random-sampling.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     actions \u001b[39m=\u001b[39m get_random_actions(env, seed)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/stefano.giannini_ama/Documents/Python/Learn/data-science_projects/Reinforcement%20Learning/LunarLander/lunarlander_random-sampling.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     best_actions \u001b[39m=\u001b[39m find_best_actions(env, actions)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/stefano.giannini_ama/Documents/Python/Learn/data-science_projects/Reinforcement%20Learning/LunarLander/lunarlander_random-sampling.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[\u001b[39m\u001b[39m{\u001b[39;00mseed\u001b[39m}\u001b[39;00m\u001b[39m] Actions length improvement:\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39mlen\u001b[39m(actions), \u001b[39m\"\u001b[39m\u001b[39m->\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(best_actions))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/stefano.giannini_ama/Documents/Python/Learn/data-science_projects/Reinforcement%20Learning/LunarLander/lunarlander_random-sampling.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     res[seed] \u001b[39m=\u001b[39m best_actions\n",
      "\u001b[1;32mc:\\Users\\stefano.giannini_ama\\Documents\\Python\\Learn\\data-science_projects\\Reinforcement Learning\\LunarLander\\lunarlander_random-sampling.ipynb Cell 6\u001b[0m in \u001b[0;36mfind_best_actions\u001b[1;34m(env, actions)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/stefano.giannini_ama/Documents/Python/Learn/data-science_projects/Reinforcement%20Learning/LunarLander/lunarlander_random-sampling.ipynb#W5sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m best_actions \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/stefano.giannini_ama/Documents/Python/Learn/data-science_projects/Reinforcement%20Learning/LunarLander/lunarlander_random-sampling.ipynb#W5sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39mwhile\u001b[39;00m change_node\u001b[39m<\u001b[39m\u001b[39mlen\u001b[39m(actions):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/stefano.giannini_ama/Documents/Python/Learn/data-science_projects/Reinforcement%20Learning/LunarLander/lunarlander_random-sampling.ipynb#W5sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     new_actions \u001b[39m=\u001b[39m explore_actions(env, change_node, actions)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/stefano.giannini_ama/Documents/Python/Learn/data-science_projects/Reinforcement%20Learning/LunarLander/lunarlander_random-sampling.ipynb#W5sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(new_actions)\u001b[39m>\u001b[39m\u001b[39mlen\u001b[39m(actions):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/stefano.giannini_ama/Documents/Python/Learn/data-science_projects/Reinforcement%20Learning/LunarLander/lunarlander_random-sampling.ipynb#W5sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m         \u001b[39m# print(len(new_actions), len(actions), change_node)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/stefano.giannini_ama/Documents/Python/Learn/data-science_projects/Reinforcement%20Learning/LunarLander/lunarlander_random-sampling.ipynb#W5sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(new_actions)\u001b[39m>\u001b[39m\u001b[39mlen\u001b[39m(best_actions):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/stefano.giannini_ama/Documents/Python/Learn/data-science_projects/Reinforcement%20Learning/LunarLander/lunarlander_random-sampling.ipynb#W5sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m             \u001b[39m# print(len(new_actions), len(actions), change_node)\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\stefano.giannini_ama\\Documents\\Python\\Learn\\data-science_projects\\Reinforcement Learning\\LunarLander\\lunarlander_random-sampling.ipynb Cell 6\u001b[0m in \u001b[0;36mexplore_actions\u001b[1;34m(env, change_node, actions)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/stefano.giannini_ama/Documents/Python/Learn/data-science_projects/Reinforcement%20Learning/LunarLander/lunarlander_random-sampling.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mfor\u001b[39;00m action \u001b[39min\u001b[39;00m actions[:\u001b[39m-\u001b[39mchange_node]:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/stefano.giannini_ama/Documents/Python/Learn/data-science_projects/Reinforcement%20Learning/LunarLander/lunarlander_random-sampling.ipynb#W5sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     step\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/stefano.giannini_ama/Documents/Python/Learn/data-science_projects/Reinforcement%20Learning/LunarLander/lunarlander_random-sampling.ipynb#W5sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     ob, rewards, terminated, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/stefano.giannini_ama/Documents/Python/Learn/data-science_projects/Reinforcement%20Learning/LunarLander/lunarlander_random-sampling.ipynb#W5sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     new_actions\u001b[39m.\u001b[39mappend(action)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/stefano.giannini_ama/Documents/Python/Learn/data-science_projects/Reinforcement%20Learning/LunarLander/lunarlander_random-sampling.ipynb#W5sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39mif\u001b[39;00m terminated: \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\stefano.giannini_ama\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\time_limit.py:18\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     15\u001b[0m     \u001b[39massert\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling reset()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 18\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     19\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     20\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Users\\stefano.giannini_ama\\Anaconda3\\lib\\site-packages\\gym\\envs\\box2d\\lunar_lander.py:350\u001b[0m, in \u001b[0;36mLunarLander.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    346\u001b[0m pos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlander\u001b[39m.\u001b[39mposition\n\u001b[0;32m    347\u001b[0m vel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlander\u001b[39m.\u001b[39mlinearVelocity\n\u001b[0;32m    348\u001b[0m state \u001b[39m=\u001b[39m [\n\u001b[0;32m    349\u001b[0m     (pos\u001b[39m.\u001b[39mx \u001b[39m-\u001b[39m VIEWPORT_W \u001b[39m/\u001b[39m SCALE \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m) \u001b[39m/\u001b[39m (VIEWPORT_W \u001b[39m/\u001b[39m SCALE \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m),\n\u001b[1;32m--> 350\u001b[0m     (pos\u001b[39m.\u001b[39my \u001b[39m-\u001b[39m (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39mhelipad_y \u001b[39m+\u001b[39m LEG_DOWN \u001b[39m/\u001b[39m SCALE)) \u001b[39m/\u001b[39m (VIEWPORT_H \u001b[39m/\u001b[39m SCALE \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m),\n\u001b[0;32m    351\u001b[0m     vel\u001b[39m.\u001b[39mx \u001b[39m*\u001b[39m (VIEWPORT_W \u001b[39m/\u001b[39m SCALE \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m) \u001b[39m/\u001b[39m FPS,\n\u001b[0;32m    352\u001b[0m     vel\u001b[39m.\u001b[39my \u001b[39m*\u001b[39m (VIEWPORT_H \u001b[39m/\u001b[39m SCALE \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m) \u001b[39m/\u001b[39m FPS,\n\u001b[0;32m    353\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlander\u001b[39m.\u001b[39mangle,\n\u001b[0;32m    354\u001b[0m     \u001b[39m20.0\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlander\u001b[39m.\u001b[39mangularVelocity \u001b[39m/\u001b[39m FPS,\n\u001b[0;32m    355\u001b[0m     \u001b[39m1.0\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlegs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mground_contact \u001b[39melse\u001b[39;00m \u001b[39m0.0\u001b[39m,\n\u001b[0;32m    356\u001b[0m     \u001b[39m1.0\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlegs[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mground_contact \u001b[39melse\u001b[39;00m \u001b[39m0.0\u001b[39m,\n\u001b[0;32m    357\u001b[0m ]\n\u001b[0;32m    358\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(state) \u001b[39m==\u001b[39m \u001b[39m8\u001b[39m\n\u001b[0;32m    360\u001b[0m reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "res = {}\n",
    "seeds = [0, 1, 5, 10, 21, 42, 47, 63, 84, 100, 121, 144]\n",
    "for seed in seeds[:2]:\n",
    "    actions = get_random_actions(env, seed)\n",
    "    best_actions = find_best_actions(env, actions)\n",
    "    print(f\"[{seed}] Actions length improvement:\",len(actions), \"->\", len(best_actions))\n",
    "    res[seed] = best_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = get_obs(env, res)\n",
    "array_obs = np.concatenate(obs[:])\n",
    "array_obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_actions = np.concatenate([np.array(act[1]) for act in res.items()][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "color = array_actions\n",
    "speed = array_obs[:, 3]\n",
    "theta = array_obs[:, 2]\n",
    "plt.figure(figsize=(7, 3), dpi=200)\n",
    "plt.scatter(theta, speed, c=color, s=1), #print(obs[:, 2].shape)\n",
    "plt.vlines([0.032, -0.032], ymin=min(speed), ymax=max(speed))\n",
    "plt.hlines([0], xmin=min(theta), xmax=max(theta))\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('omega')\n",
    "plt.colorbar();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lbc = LGBMClassifier(n_estimators=100)\n",
    "\n",
    "lbc.fit(array_obs, array_actions)\n",
    "accuracy_score(array_actions, lbc.predict(array_obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbc.feature_importances_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model on environment\n",
    "It suuucks, I saved *next_obs* and not the actual obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = []\n",
    "for seed in range(1, 200, 5):\n",
    "    env.seed(seed)\n",
    "    ob = env.reset()\n",
    "\n",
    "    step = 0\n",
    "    while True:\n",
    "        # print(ob.reshape(1, -1).shape)\n",
    "        action = lbc.predict(ob.reshape(1, -1))[0]\n",
    "        # action = res[seeds[4]][step]\n",
    "        ob, reward, terminated, info = env.step(action)\n",
    "        step+=1\n",
    "        if terminated: steps.append(step);break\n",
    "        \n",
    "    print(seed, ':',step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(steps), np.mean(steps), np.min(steps), np.max(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "929ea3a73902e04b651b50d7e8bff86e69e28c3f97df2b308a538a481df81bfd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
